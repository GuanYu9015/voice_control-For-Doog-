# 模型路徑設定
# 所有路徑相對於 models/ 目錄

# DNS 語音增強/噪音抑制設定
dns:
  # 後端選擇: auto / deepfilternet / onnx / passthrough
  # auto: 優先使用 DeepFilterNet，若不可用則使用 ONNX 或 Passthrough
  backend: auto
  
  # DeepFilterNet 設定（推薦，效果最好）
  deepfilternet:
    model: DeepFilterNet3  # DeepFilterNet / DeepFilterNet2 / DeepFilterNet3
    post_filter: false     # 後處理濾波器（可更強降噪但可能影響語音品質）
    atten_lim_db: 100.0    # 最大衰減量 (dB)
    streaming: true        # 使用串流模式（即時處理）
  
  # ONNX 模型設定（備用方案）
  onnx:
    model_path: dns/nsnet2-20ms-baseline.onnx

# VAD 模型 (Silero VAD)
vad:
  model_path: vad/silero_vad.onnx

# KWS 喚醒詞模型
kws:
  model_path: kws/sherpa-onnx-kws-zipformer-wenetspeech-3.3M
  tokens_path: kws/tokens.txt
  keywords_file: kws/keywords.txt

# ASR 語音辨識模型 (中文)
asr:
  # Streaming ASR (即時辨識) - 使用 Streaming Zipformer
  streaming:
    encoder: asr/sherpa-onnx-streaming-zipformer-zh/encoder-epoch-99-avg-1.onnx
    decoder: asr/sherpa-onnx-streaming-zipformer-zh/decoder-epoch-99-avg-1.onnx
    joiner: asr/sherpa-onnx-streaming-zipformer-zh/joiner-epoch-99-avg-1.onnx
    tokens: asr/sherpa-onnx-streaming-zipformer-zh/tokens.txt
  
  # Offline ASR (離線辨識，精度較高)
  offline:
    model_path: asr/sherpa-onnx-paraformer-zh
    tokens: asr/sherpa-onnx-paraformer-zh/tokens.txt

# TTS 語音合成模型 (中文)
tts:
  model_path: tts/vits-zh-aishell3
  lexicon: tts/vits-zh-aishell3/lexicon.txt
  tokens: tts/vits-zh-aishell3/tokens.txt

# LLM 模型設定
llm:
  # 後端選擇: auto / llama_cpp / nanollm / mock
  # auto: 優先 llama.cpp，若模型不存在則使用 NanoLLM
  backend: llama_cpp
  
  # llama.cpp 設定 (推薦 - 支援新模型)
  llama_cpp:
    # GGUF 模型路徑（相對於 models/llm/ 或絕對路徑）
    model_path: models/llm/Qwen2.5-7B-Instruct-Q4_K_M.gguf
    n_ctx: 8192          # 上下文長度（避免 token 溢出）
    n_gpu_layers: -1     # GPU 層數 (-1 = 全部)
    
    # 可用模型列表
    available_models:
      - name: qwen2.5-3b
        file: qwen2.5-3b-instruct-q4_k_m.gguf
        url: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf
        size_gb: 2.0
        lang: 中文
      - name: qwen2.5-7b
        file: Qwen2.5-7B-Instruct-Q4_K_M.gguf
        url: https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf
        size_gb: 4.7
        lang: 中文
      - name: llama-3.2-3b
        file: llama-3.2-3b-instruct-q4_k_m.gguf
        url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
        size_gb: 2.0
        lang: 英文/多語言
      # 台灣中文模型
      - name: llama-3.2-tw-3b
        file: Llama-3.2-Taiwan-3B-Instruct.Q4_K_M.gguf
        url: https://huggingface.co/QuantFactory/Llama-3.2-Taiwan-3B-Instruct-GGUF/resolve/main/Llama-3.2-Taiwan-3B-Instruct.Q4_K_M.gguf
        size_gb: 2.0
        lang: 台灣中文
        note: yentinglin/Llama-3.2-TW-3B 的 GGUF 版本
      - name: taide-llama-3-8b
        file: Llama3-TAIDE-LX-8B-Chat-Alpha1-Q4_K_M.gguf
        url: https://huggingface.co/QuantFactory/Llama3-TAIDE-LX-8B-Chat-Alpha1-GGUF/resolve/main/Llama3-TAIDE-LX-8B-Chat-Alpha1.Q4_K_M.gguf
        size_gb: 5.0
        lang: 台灣中文
        note: TAIDE 計畫開發的台灣繁體中文大模型
      - name: taide-llama-3.1-8b
        file: Llama-3.1-TAIDE-LX-8B-Chat-Q4_K_M.gguf
        url: https://huggingface.co/tetf/Llama-3.1-TAIDE-LX-8B-Chat-GGUF/resolve/main/Llama-3.1-TAIDE-LX-8B-Chat-Q4_K_M.gguf
        size_gb: 5.0
        lang: 台灣中文
        note: TAIDE Llama 3.1 版本
  
  # NanoLLM/MLC 設定 (最快，但模型支援有限)
  nanollm:
    # MLC 支援的模型（JetPack 5.12）
    available_models:
      - name: llama-2-7b
        path: meta-llama/Llama-2-7b-chat-hf
        quantization: q4f16_ft
      - name: vicuna-7b
        path: lmsys/vicuna-7b-v1.5
        quantization: q4f16_ft
      - name: chinese-alpaca-2-7b
        path: ziqingyang/chinese-alpaca-2-7b
        quantization: q4f16_ft
    default_model: llama-2-7b
  
  # 推理參數
  max_tokens: 256
  temperature: 0.7
  top_p: 0.9
  
  # 護理車 Prompt 設定
  # 注意：護理車使用動態 Prompt，定義在 src/robot/nursing_commands.py
  # 的 build_nursing_cart_prompt() 函式中
